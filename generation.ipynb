{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRSQy1XhJcyX"
      },
      "source": [
        "# Loading necessary data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UXPtr_4qYqJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "!git clone https://github.com/RegNLP/ObliQADataset.git\n",
        "\n",
        "# Downloading the documents and extracting their passages\n",
        "documents_path = \"ObliQADataset/StructuredRegulatoryDocuments\"\n",
        "\n",
        "docs = []\n",
        "for i in range(1, 41):\n",
        "    file_path = os.path.join(documents_path, f'{i}.json')\n",
        "    with open(file_path, 'r') as file:\n",
        "        docs.append(json.load(file))\n",
        "\n",
        "passages = [passage for doc in docs for passage in doc]\n",
        "\n",
        "print(f'Loaded 40 documents containing {len(passages)} passages.')\n",
        "\n",
        "with open(\"ObliQADataset/RIRAGSharedTask/RIRAG_Unseen_Questions.json\") as f:\n",
        "    questions = json.load(f)\n",
        "\n",
        "print(f'Loaded question JSON containing {len(questions)} questions.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU2CmB0rH--Q"
      },
      "outputs": [],
      "source": [
        "# Generating dictionaries for efficient lookup\n",
        "psgid2index = {}\n",
        "for i, psg in enumerate(passages):\n",
        "    if psg['ID'] not in psgid2index:\n",
        "        psgid2index[psg['ID']] = i\n",
        "\n",
        "qid2question = {q['QuestionID'] : q['Question'] for q in questions}\n",
        "\n",
        "pid2passage = {}\n",
        "for psg in passages:\n",
        "    if psg['ID'] not in pid2passage:\n",
        "        pid2passage[psg['ID']] = psg['Passage']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrZXM5_NPWuS"
      },
      "outputs": [],
      "source": [
        "# Downloading the ObligationClassificationDataset\n",
        "\n",
        "# URL of the raw JSON file\n",
        "file_url = \"https://raw.githubusercontent.com/RegNLP/ObligationClassifier/main/ObligationClassificationDataset.json\"\n",
        "file_path = \"ObligationClassificationDataset.json\"\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(file_path):\n",
        "    print(\"File doesn't exist, downloading...\")\n",
        "\n",
        "    # Send a GET request to download the file\n",
        "    response = requests.get(file_url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Save the content to a local file\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(\"File downloaded successfully!\")\n",
        "    else:\n",
        "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
        "else:\n",
        "    print(\"File already exists.\")\n",
        "\n",
        "# Load the downloaded JSON file\n",
        "with open(file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "print(f\"Loaded data into {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE9lMiPtwcn_"
      },
      "outputs": [],
      "source": [
        "folder_path = \"\"  # The path which includes \"ObligationClassificationDataset.json\" and the \"rankings.trec\" file containing your retrieval results\n",
        "\n",
        "# Check if results directory exists\n",
        "if not os.path.exists('res'):\n",
        "    print(f\"Directory \\'res\\' does not exist. Creating it...\")\n",
        "    os.makedirs('res')  # Create the directory\n",
        "    print(f\"Directory \\'res\\' created successfully!\")\n",
        "else:\n",
        "    print(f\"Directory \\'res\\' already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAJgbKebH--S"
      },
      "source": [
        "# Base code from RegNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CB9YkVGwYwx"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCFwjCc-whWe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    pipeline,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from nltk.tokenize import sent_tokenize as sent_tokenize_uncached\n",
        "import nltk\n",
        "from functools import cache\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set up random seeds and deterministic flags for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Check if CUDA is available and use it if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Step 2: Load and preprocess the data\n",
        "json_path = os.path.join(folder_path, \"ObligationClassificationDataset.json\")\n",
        "with open(json_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "texts = [item['Text'] for item in data]\n",
        "labels = [1 if item['Obligation'] else 0 for item in data]  # Converting True/False to 1/0\n",
        "\n",
        "# Step 3: Tokenization using LegalBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
        "\n",
        "class ObligationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = ObligationDataset(X_train, y_train, tokenizer)\n",
        "val_dataset = ObligationDataset(X_val, y_val, tokenizer)\n",
        "\n",
        "# Step 4: Fine-tuning LegalBERT for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'nlpaueb/legal-bert-base-uncased', num_labels=2\n",
        ")\n",
        "model.to(device)  # Move model to the GPU\n",
        "\n",
        "# Ensure the directories exist for saving results and logs\n",
        "output_dir = os.path.join(folder_path, 'results')\n",
        "log_dir = os.path.join(folder_path, 'logs')\n",
        "save_dir = os.path.join(folder_path, 'obligation-classifier-legalbert')\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=log_dir,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    seed=42,  # Set seed in TrainingArguments\n",
        ")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average='binary'\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")\n",
        "\n",
        "# Step 5: Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "trainer.evaluate()\n",
        "\n",
        "# Step 7: Save the model and tokenizer for future use\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(\"Model fine-tuning and evaluation completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22aer826Gdkl"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load the tokenizer and model for obligation detection\n",
        "model_name = os.path.join(folder_path, 'obligation-classifier-legalbert')\n",
        "obligation_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "obligation_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "obligation_model.to(device)\n",
        "obligation_model.eval()\n",
        "\n",
        "# Load NLI model and tokenizer for obligation coverage\n",
        "coverage_nli_model = pipeline(\n",
        "    \"text-classification\", model=\"microsoft/deberta-large-mnli\", device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Load NLI model and tokenizer for entailment and contradiction checks\n",
        "nli_tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-xsmall')\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-xsmall')\n",
        "nli_model.to(device)\n",
        "nli_model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
        "\n",
        "# Define a cached version of sentence tokenization\n",
        "@cache\n",
        "def sent_tokenize(passage: str):\n",
        "    return sent_tokenize_uncached(passage)\n",
        "\n",
        "def softmax(logits):\n",
        "    e_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "    return e_logits / np.sum(e_logits, axis=1, keepdims=True)\n",
        "\n",
        "def get_nli_probabilities(premises, hypotheses):\n",
        "    features = nli_tokenizer(\n",
        "        premises,\n",
        "        hypotheses,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    nli_model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = nli_model(**features).logits.cpu().numpy()\n",
        "    probabilities = softmax(logits)\n",
        "    return probabilities\n",
        "\n",
        "def get_nli_matrix(passages, answers):\n",
        "    entailment_matrix = np.zeros((len(passages), len(answers)))\n",
        "    contradiction_matrix = np.zeros((len(passages), len(answers)))\n",
        "\n",
        "    batch_size = 16\n",
        "    for i, pas in enumerate(passages):\n",
        "        for b in range(0, len(answers), batch_size):\n",
        "            e = b + batch_size\n",
        "            probs = get_nli_probabilities(\n",
        "                [pas] * len(answers[b:e]), answers[b:e]\n",
        "            )  # Get NLI probabilities\n",
        "            entailment_matrix[i, b:e] = probs[:, 1]\n",
        "            contradiction_matrix[i, b:e] = probs[:, 0]\n",
        "    return entailment_matrix, contradiction_matrix\n",
        "\n",
        "def calculate_scores_from_matrix(nli_matrix, score_type='entailment'):\n",
        "    if nli_matrix.size == 0:\n",
        "        return 0.0  # or some other default score or handling as appropriate for your use case\n",
        "\n",
        "    if score_type == 'entailment':\n",
        "        reduced_vector = np.max(nli_matrix, axis=0)\n",
        "    elif score_type == 'contradiction':\n",
        "        reduced_vector = np.max(nli_matrix, axis=0)\n",
        "    score = np.round(np.mean(reduced_vector), 5)\n",
        "    return score\n",
        "\n",
        "def classify_obligations(sentences):\n",
        "    inputs = obligation_tokenizer(\n",
        "        sentences, padding=True, truncation=True, return_tensors='pt'\n",
        "    ).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = obligation_model(**inputs).logits\n",
        "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    return predictions\n",
        "\n",
        "def calculate_obligation_coverage_score(passages, answers):\n",
        "    # Filter obligation sentences from passages\n",
        "    obligation_sentences_source = []\n",
        "    for passage in passages:\n",
        "        sentences = sent_tokenize(passage)\n",
        "        is_obligation = classify_obligations(sentences)\n",
        "        obligation_sentences_source.extend(\n",
        "            [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "        )\n",
        "\n",
        "    # Filter obligation sentences from answers\n",
        "    obligation_sentences_answer = []\n",
        "    for answer in answers:\n",
        "        sentences = sent_tokenize(answer)\n",
        "        is_obligation = classify_obligations(sentences)\n",
        "        obligation_sentences_answer.extend(\n",
        "            [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "        )\n",
        "\n",
        "    # Calculate coverage based on NLI entailment\n",
        "    covered_count = 0\n",
        "    for obligation in obligation_sentences_source:\n",
        "        for answer_sentence in obligation_sentences_answer:\n",
        "            nli_result = coverage_nli_model(\n",
        "                f\"{answer_sentence} [SEP] {obligation}\"\n",
        "            )\n",
        "            if nli_result[0]['label'].lower() == 'entailment' and nli_result[0]['score'] > 0.7:\n",
        "                covered_count += 1\n",
        "                break\n",
        "\n",
        "    return (\n",
        "        covered_count / len(obligation_sentences_source)\n",
        "        if obligation_sentences_source\n",
        "        else 0\n",
        "    )\n",
        "\n",
        "def calculate_final_composite_score(passages, answers):\n",
        "    passage_sentences = [sent for passage in passages for sent in sent_tokenize(passage)]\n",
        "    answer_sentences = [sent for answer in answers for sent in sent_tokenize(answer)]\n",
        "\n",
        "    # Calculate NLI matrix for entailment and contradiction\n",
        "    entailment_matrix, contradiction_matrix = get_nli_matrix(\n",
        "        passage_sentences, answer_sentences\n",
        "    )\n",
        "\n",
        "    # Calculate scores\n",
        "    entailment_score = calculate_scores_from_matrix(entailment_matrix, 'entailment')\n",
        "    contradiction_score = calculate_scores_from_matrix(\n",
        "        contradiction_matrix, 'contradiction'\n",
        "    )\n",
        "    obligation_coverage_score = calculate_obligation_coverage_score(passages, answers)\n",
        "\n",
        "    # Final composite score formula\n",
        "    composite_score = (\n",
        "        obligation_coverage_score + entailment_score - contradiction_score + 1\n",
        "    ) / 3\n",
        "\n",
        "    # Return all scores\n",
        "    return (\n",
        "        np.round(composite_score, 5),\n",
        "        entailment_score,\n",
        "        contradiction_score,\n",
        "        obligation_coverage_score,\n",
        "    )\n",
        "\n",
        "def calculate_average_scores_from_csv(output_file_csv):\n",
        "    \"\"\"Calculate average scores from the CSV file.\"\"\"\n",
        "    entailment_scores = []\n",
        "    contradiction_scores = []\n",
        "    obligation_coverage_scores = []\n",
        "    composite_scores = []\n",
        "\n",
        "    with open(output_file_csv, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            try:\n",
        "                entailment_scores.append(float(row['entailment_score']))\n",
        "                contradiction_scores.append(float(row['contradiction_score']))\n",
        "                obligation_coverage_scores.append(float(row['obligation_coverage_score']))\n",
        "                composite_scores.append(float(row['composite_score']))\n",
        "            except ValueError:\n",
        "                # Handle the case where the value cannot be converted to float\n",
        "                print(f\"Skipping invalid row: {row}\")\n",
        "\n",
        "    avg_entailment = np.mean(entailment_scores) if entailment_scores else 0.0\n",
        "    avg_contradiction = np.mean(contradiction_scores) if contradiction_scores else 0.0\n",
        "    avg_obligation_coverage = (\n",
        "        np.mean(obligation_coverage_scores) if obligation_coverage_scores else 0.0\n",
        "    )\n",
        "    avg_composite = np.mean(composite_scores) if composite_scores else 0.0\n",
        "\n",
        "    return avg_entailment, avg_contradiction, avg_obligation_coverage, avg_composite\n",
        "\n",
        "def main(input_file_path, group_method_name):\n",
        "    # Create a directory with the group_method_name in the folder path\n",
        "    output_dir = os.path.join(folder_path, group_method_name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Define the paths for result files\n",
        "    output_file_csv = os.path.join(output_dir, 'results.csv')\n",
        "    output_file_txt = os.path.join(output_dir, 'results.txt')\n",
        "\n",
        "    processed_question_ids = set()\n",
        "    saved_items_count = 0\n",
        "\n",
        "    # Check if the output CSV file already exists and read processed QuestionIDs\n",
        "    if os.path.exists(output_file_csv):\n",
        "        with open(output_file_csv, 'r') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                processed_question_ids.add(row['QuestionID'])\n",
        "                saved_items_count += 1\n",
        "\n",
        "    with open(input_file_path, 'r') as file:\n",
        "        test_data = json.load(file)\n",
        "\n",
        "    total_items = len(test_data)\n",
        "\n",
        "    # Open the CSV file for appending results\n",
        "    with open(output_file_csv, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        if not processed_question_ids:\n",
        "            # Write the header if the file is empty or new\n",
        "            writer.writerow(\n",
        "                [\n",
        "                    'QuestionID',\n",
        "                    'entailment_score',\n",
        "                    'contradiction_score',\n",
        "                    'obligation_coverage_score',\n",
        "                    'composite_score',\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        for index, item in enumerate(test_data, start=1):\n",
        "            question_id = item['QuestionID']\n",
        "\n",
        "            # Skip if the QuestionID has already been processed\n",
        "            if question_id in processed_question_ids:\n",
        "                continue\n",
        "\n",
        "            # Skip if the \"Answer\" is null or empty\n",
        "            if not item.get('Answer') or not item['Answer'].strip():\n",
        "                continue\n",
        "\n",
        "            # Merge \"RetrievedPassages\" if it's a list\n",
        "            if isinstance(item['RetrievedPassages'], list):\n",
        "                item['RetrievedPassages'] = \" \".join(item['RetrievedPassages'])\n",
        "\n",
        "            passages = [item['RetrievedPassages']]\n",
        "            answers = [item['Answer']]\n",
        "            (\n",
        "                composite_score,\n",
        "                entailment_score,\n",
        "                contradiction_score,\n",
        "                obligation_coverage_score,\n",
        "            ) = calculate_final_composite_score(passages, answers)\n",
        "\n",
        "            # Write the result to the CSV file\n",
        "            writer.writerow(\n",
        "                [\n",
        "                    question_id,\n",
        "                    entailment_score,\n",
        "                    contradiction_score,\n",
        "                    obligation_coverage_score,\n",
        "                    composite_score,\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Increment the saved items count and print status\n",
        "            saved_items_count += 1\n",
        "            print(f\"{saved_items_count}/{total_items}\")\n",
        "\n",
        "    # Calculate average scores from the CSV file\n",
        "    (\n",
        "        avg_entailment,\n",
        "        avg_contradiction,\n",
        "        avg_obligation_coverage,\n",
        "        avg_composite,\n",
        "    ) = calculate_average_scores_from_csv(output_file_csv)\n",
        "\n",
        "    # Print and save results to a text file\n",
        "    results = (\n",
        "        f\"Average Entailment Score: {avg_entailment}\\n\"\n",
        "        f\"Average Contradiction Score: {avg_contradiction}\\n\"\n",
        "        f\"Average Obligation Coverage Score: {avg_obligation_coverage}\\n\"\n",
        "        f\"Average Final Composite Score: {avg_composite}\\n\"\n",
        "    )\n",
        "\n",
        "    print(results)\n",
        "\n",
        "    with open(output_file_txt, 'w') as txtfile:\n",
        "        txtfile.write(results)\n",
        "\n",
        "    print(f\"Processing complete. Results saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksknBabnH--b"
      },
      "source": [
        "# Extending the base code for our implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bMrubw-H--c"
      },
      "outputs": [],
      "source": [
        "def parse_trec_file(file_path: str):\n",
        "    \"\"\"\n",
        "    Recreates the retrieved dictionary that represents the results of the retrieval stage\n",
        "    \"\"\"\n",
        "    trec_dict = {}\n",
        "\n",
        "    # open and read the TREC file line by line\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            question_id = parts[0]\n",
        "            passage_id = parts[2]\n",
        "            score = float(parts[4])\n",
        "\n",
        "            # if the QuestionID is not in the dictionary, add it\n",
        "            if question_id not in trec_dict:\n",
        "                trec_dict[question_id] = []\n",
        "\n",
        "            trec_dict[question_id].append((passage_id, score))\n",
        "\n",
        "    return trec_dict\n",
        "\n",
        "# Functions needed for VRR's refine step:\n",
        "\n",
        "def calculate_average(dataset: list):\n",
        "    # Find average contradiction score using RePASs methods\n",
        "    contradiction_scores = []\n",
        "    for item in tqdm(dataset):\n",
        "      passages = [\" \".join(item['RetrievedPassages'])]\n",
        "      answers = [item['Answer']]\n",
        "\n",
        "      passage_sentences = [sent for passage in passages for sent in sent_tokenize(passage)]\n",
        "      answer_sentences = [sent for answer in answers for sent in sent_tokenize(answer)]\n",
        "\n",
        "      # Calculate NLI matrix for entailment and contradiction\n",
        "      entailment_matrix, contradiction_matrix = get_nli_matrix(\n",
        "          passage_sentences, answer_sentences\n",
        "      )\n",
        "\n",
        "      # Calculate contradiction score\n",
        "      contradiction_score = calculate_scores_from_matrix(contradiction_matrix, 'contradiction')\n",
        "\n",
        "      contradiction_scores.append(contradiction_score)\n",
        "\n",
        "    avg_contradiction = np.mean(contradiction_scores) if contradiction_scores else 0.1\n",
        "\n",
        "    return avg_contradiction\n",
        "\n",
        "def get_obligation_covering_sentences(passages, answers):\n",
        "    # Filter obligation sentences from passages\n",
        "    obligation_sentences_source = []\n",
        "    for passage in passages:\n",
        "        sentences = sent_tokenize(passage)\n",
        "        is_obligation = classify_obligations(sentences)\n",
        "        obligation_sentences_source.extend(\n",
        "            [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "        )\n",
        "\n",
        "    # Filter obligation sentences from answers\n",
        "    obligation_sentences_answer = []\n",
        "    for answer in answers:\n",
        "        sentences = sent_tokenize(answer)\n",
        "        is_obligation = classify_obligations(sentences)\n",
        "        obligation_sentences_answer.extend(\n",
        "            [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "        )\n",
        "\n",
        "    # Find sentences from answers that cover obligations in passages\n",
        "    covering_sentences = []\n",
        "    for answer_sentence in obligation_sentences_answer:\n",
        "        for obligation in obligation_sentences_source:\n",
        "            nli_result = coverage_nli_model(\n",
        "                f\"{answer_sentence} [SEP] {obligation}\"\n",
        "            )\n",
        "            if nli_result[0]['label'].lower() == 'entailment' and nli_result[0]['score'] > 0.7:\n",
        "                covering_sentences.append(answer_sentence)\n",
        "                break\n",
        "\n",
        "    return covering_sentences\n",
        "\n",
        "def get_uncovered_obligations(passages, answers):\n",
        "    # Filter obligation sentences from passages\n",
        "    obligation_sentences_source = []\n",
        "    for passage in passages:\n",
        "        sentences = sent_tokenize(passage)\n",
        "        is_obligation = classify_obligations(sentences)\n",
        "        obligation_sentences_source.extend(\n",
        "            [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "        )\n",
        "\n",
        "    # Check if answers contain any meaningful content\n",
        "    meaningful_answers = [\n",
        "        answer.strip() for answer in answers if answer.strip()\n",
        "    ]  # Remove empty or whitespace-only answers\n",
        "    if not meaningful_answers:\n",
        "        return obligation_sentences_source\n",
        "\n",
        "    # Filter obligation sentences from answers\n",
        "    obligation_sentences_answer = []\n",
        "    for answer in answers:\n",
        "        sentences = sent_tokenize(answer)\n",
        "        is_obligation = classify_obligations(sentences)\n",
        "        obligation_sentences_answer.extend(\n",
        "            [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "        )\n",
        "\n",
        "    # Find uncovered obligations based on NLI entailment\n",
        "    checklist = [False for i in obligation_sentences_source]\n",
        "    for index, obligation in enumerate(obligation_sentences_source):\n",
        "        for answer_sentence in obligation_sentences_answer:\n",
        "            nli_result = coverage_nli_model(\n",
        "                f\"{answer_sentence} [SEP] {obligation}\"\n",
        "            )\n",
        "            if nli_result[0]['label'].lower() == 'entailment' and nli_result[0]['score'] > 0.7:\n",
        "                checklist[index] = True\n",
        "                break\n",
        "\n",
        "    return [obl for flag, obl in zip(checklist, obligation_sentences_source) if flag==False]\n",
        "\n",
        "# Helper function for LOC\n",
        "\n",
        "def is_covered(obligations, answers):\n",
        "    # Filter obligation sentences\n",
        "    obligation_sentences_source = []\n",
        "    for obligation in obligations:\n",
        "        sentences = sent_tokenize(obligation)\n",
        "        is_obligation = classify_obligations(sentences)\n",
        "        obligation_sentences_source.extend(\n",
        "            [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "        )\n",
        "\n",
        "    # Filter obligation sentences from answers\n",
        "    obligation_sentences_answer = []\n",
        "    for answer in answers:\n",
        "        sentences = sent_tokenize(answer)\n",
        "        is_obligation = classify_obligations(sentences)\n",
        "        obligation_sentences_answer.extend(\n",
        "            [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "        )\n",
        "\n",
        "    # Check for coverage using NLI entailment\n",
        "    for obligation in obligation_sentences_source:\n",
        "        for answer_sentence in obligation_sentences_answer:\n",
        "            nli_result = coverage_nli_model(\n",
        "                f\"{answer_sentence} [SEP] {obligation}\"\n",
        "            )\n",
        "            if nli_result[0]['label'].lower() == 'entailment' and nli_result[0]['score'] > 0.7:\n",
        "                return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7uAlyPdH--f"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_api_key = \"\" # your OpenAI API key\n",
        "\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "GPT_MODEL = \"gpt-4o-mini\" # \"gpt-4o-2024-08-06\" # you can also use \"gpt-4o-mini\"\n",
        "\n",
        "with open(\"prompts.json\") as f:\n",
        "    prompts = json.load(f)\n",
        "\n",
        "def generate_prompt(input_text: str, context, context_type: str = 'Passages', input_type: str = 'Question')->str:\n",
        "\n",
        "  if isinstance(context, str):\n",
        "        context = [context]\n",
        "  joined_context = \"\\n\".join(context)\n",
        "\n",
        "  return f\"\"\"{context_type}:\n",
        "  {joined_context}\n",
        "\n",
        "  {input_type}:\n",
        "  {input_text}\"\"\"\n",
        "\n",
        "def get_answer(user_prompt: str, system_prompt: str)->str:\n",
        "    response = client.chat.completions.create(\n",
        "    model=GPT_MODEL,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": system_prompt\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": user_prompt\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "    ],\n",
        "      temperature=1,\n",
        "      top_p=1\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEQLBwkJJCk9"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7yrC-cDkfV6"
      },
      "outputs": [],
      "source": [
        "def keep_qualifying(psg_tuples, threshold = 0.7, max_drop = 0.2, keep_one=True):\n",
        "    first = psg_tuples[0]\n",
        "    # Keep only the passages with high relevance\n",
        "    for i in range(len(psg_tuples)):\n",
        "        if psg_tuples[i][1] < threshold :\n",
        "            psg_tuples = psg_tuples[:i]\n",
        "            break\n",
        "\n",
        "    # Keep only the passages for which no significant drop was observed\n",
        "    for i in range(1, len(psg_tuples)):\n",
        "        if psg_tuples[i-1][1] - psg_tuples[i][1] >= max_drop:\n",
        "            psg_tuples = psg_tuples[:i]\n",
        "            break\n",
        "\n",
        "    # Keep at least one answer if none satisfy the criteria\n",
        "    if not psg_tuples and keep_one: psg_tuples.append(first)\n",
        "\n",
        "    return psg_tuples\n",
        "\n",
        "def preprocess(retrieval_results: dict, threshold: float = 0.7, max_drop: float = 0.2, keep_obligations: bool = True)->list:\n",
        "\n",
        "    filtered = {}\n",
        "    for qid, hit in retrieval_results.items():\n",
        "      qualified = keep_qualifying(hit, threshold=threshold, max_drop=max_drop, keep_one=True)\n",
        "      filtered[qid] = qualified\n",
        "\n",
        "    # Perform preprocessing\n",
        "    preprocessed = []\n",
        "    for qid, hit in tqdm(filtered.items()):\n",
        "      question = qid2question[qid]\n",
        "      passages = [pid2passage[t[0]] for t in hit]\n",
        "\n",
        "      preprocessed.append(\n",
        "            {\n",
        "            \"QuestionID\": qid,\n",
        "            \"Question\": question,\n",
        "            \"RetrievedPassages\": passages,\n",
        "            }\n",
        "        )\n",
        "\n",
        "      if keep_obligations:\n",
        "\n",
        "        # For each passage keep only the sentences that are labeled as obligations\n",
        "        obligations = []\n",
        "        for passage in passages:\n",
        "          sentences = sent_tokenize(passage)\n",
        "          if not sentences: continue\n",
        "          is_obligation = classify_obligations(sentences)\n",
        "          obligation_sents = [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "          if obligation_sents:\n",
        "            obligations.extend(obligation_sents)\n",
        "\n",
        "        # If there are not any obligations keep the passage as it is\n",
        "        if not obligations:\n",
        "          obligations=passages\n",
        "\n",
        "        preprocessed[-1][\"Obligations\"] = obligations\n",
        "\n",
        "    return preprocessed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIWfNdmwJAHh"
      },
      "source": [
        "# VRR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1-7lm4Oku-A"
      },
      "outputs": [],
      "source": [
        "def verify(item: dict, N: int, optimization_variable: str)->dict:\n",
        "\n",
        "    # Get N different answers for each question\n",
        "    item['Answers'] = []\n",
        "    for i in range(N):\n",
        "        user_prompt = generate_prompt(input_text=item['Question'], context=item['Obligations'], context_type='Obligations')\n",
        "        answer = get_answer(user_prompt=user_prompt, system_prompt=prompts['Obligations Context Prompt'])\n",
        "        item['Answers'].append(answer)\n",
        "\n",
        "\n",
        "    # Pick the answer with the best optimization variable score\n",
        "    passages = [\" \".join(item['RetrievedPassages'])]\n",
        "\n",
        "    scores_repass = []\n",
        "    scores_entailment = []\n",
        "    scores_contradiction = []\n",
        "    scores_obligations = []\n",
        "    for answer in item['Answers']:\n",
        "        answers = [answer]\n",
        "        (\n",
        "            composite_score,\n",
        "            entailment_score,\n",
        "            contradiction_score,\n",
        "            obligation_coverage_score,\n",
        "        ) = calculate_final_composite_score(passages, answers)\n",
        "        scores_repass.append(composite_score)\n",
        "        scores_entailment.append(entailment_score)\n",
        "        scores_contradiction.append(contradiction_score)\n",
        "        scores_obligations.append(obligation_coverage_score)\n",
        "\n",
        "\n",
        "    if optimization_variable == 'repass':\n",
        "        item['Answer'] = item['Answers'][np.argmax(scores_repass)]\n",
        "\n",
        "    elif optimization_variable == 'entailment':\n",
        "        item['Answer'] = item['Answers'][np.argmax(scores_entailment)]\n",
        "\n",
        "    elif optimization_variable == 'contradiction':\n",
        "        item['Answer'] = item['Answers'][np.argmin(scores_contradiction)]\n",
        "\n",
        "    elif optimization_variable == 'obligations':\n",
        "        item['Answer'] = item['Answers'][np.argmax(scores_obligations)]\n",
        "\n",
        "    return item\n",
        "\n",
        "def refine(item: dict, average=0.02, obl_injection: bool = True)->dict:\n",
        "\n",
        "    # Remove any sentence higher than the average that do not cover an obligation\n",
        "    passages = [\" \".join(item['RetrievedPassages'])]\n",
        "    answers = [item['Answer']]\n",
        "\n",
        "    passage_sentences = [sent for passage in passages for sent in sent_tokenize(passage)]\n",
        "    answer_sentences = [sent for answer in answers for sent in sent_tokenize(answer)]\n",
        "\n",
        "    # Calculate NLI matrix for entailment and contradiction\n",
        "    entailment_matrix, contradiction_matrix = get_nli_matrix(\n",
        "        passage_sentences, answer_sentences\n",
        "    )\n",
        "\n",
        "    # Identify high contradiction sentences\n",
        "    reduced_sents_vector = np.max(contradiction_matrix, axis=0)\n",
        "    high_contradiction_indices = np.where(reduced_sents_vector > average)[0].tolist()\n",
        "\n",
        "    meaningful_answers = [answer.strip() for answer in answers if answer.strip()]  # Remove empty or whitespace-only answers\n",
        "\n",
        "    # Identrify sentences that cover obligations\n",
        "    covering_sentences = get_obligation_covering_sentences(passages, answers)\n",
        "\n",
        "    new_answer = item['Answer']\n",
        "    for hci in high_contradiction_indices:\n",
        "      if answer_sentences[hci] not in covering_sentences: # Remove only if the sentence does not cover an obligation\n",
        "        new_answer = new_answer.replace(answer_sentences[hci], \"\")\n",
        "\n",
        "    item['Answer'] = new_answer\n",
        "\n",
        "    if obl_injection:\n",
        "      # Extract obligations from passages and find which obligations are not covered\n",
        "\n",
        "      passages = [\" \".join(item['RetrievedPassages'])]\n",
        "      answers = [item['Answer']]\n",
        "      uncovered_obligations = get_uncovered_obligations(passages, answers)\n",
        "      item['RemainingObligations'] = uncovered_obligations\n",
        "\n",
        "      meaningful_answers = [answer.strip() for answer in answers if answer.strip()]  # Remove empty or whitespace-only answers\n",
        "      if not meaningful_answers and not item['RemainingObligations'] :\n",
        "        item['RemainingObligations'] = item['RetrievedPassages']\n",
        "\n",
        "      # Prompt gpt-4o to adapt or add sentences in order to include all missing obligations if there are any\n",
        "      if item['RemainingObligations'] :\n",
        "        user_prompt = generate_prompt(input_text=item['Answer'], context=item['RemainingObligations'], context_type='Obligations', input_type='Answer')\n",
        "        item['Answer'] = get_answer(user_prompt=user_prompt, system_prompt=prompts['Obligation Insertion Prompt'])\n",
        "\n",
        "    return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8Kg-AZyksBV"
      },
      "outputs": [],
      "source": [
        "retrieval_results = parse_trec_file('rankings.trec')\n",
        "preprocessed = preprocess(retrieval_results, threshold=0.9, max_drop=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd20YpRPwYnl"
      },
      "outputs": [],
      "source": [
        "for item in preprocessed:\n",
        "    item = verify(item=item, N=5, optimization_variable='repass')\n",
        "\n",
        "for i in range(3):\n",
        "    dataset_avg = calculate_average(preprocessed)\n",
        "    for item in preprocessed:\n",
        "        item = refine(item=item, average=dataset_avg)\n",
        "\n",
        "    if i == 2:\n",
        "        item = refine(item=item, average=dataset_avg, obl_injection=False)\n",
        "\n",
        "output_data = []\n",
        "for item in preprocessed:\n",
        "    output_data.append({\n",
        "        \"QuestionID\": item['QuestionID'],\n",
        "        \"Question\": item['Question'],\n",
        "        \"RetrievedPassages\": item['RetrievedPassages'],\n",
        "        \"Answer\": item['Answer']\n",
        "    })\n",
        "\n",
        "filepath = 'res'\n",
        "filename = 'VRR_results.json'\n",
        "# Write the results to a JSON file\n",
        "with open(filepath+'/'+filename, \"w\") as outfile:\n",
        "    json.dump(output_data, outfile, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy75Ldxkxb1U"
      },
      "source": [
        "# NOC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rict8Wj0xeqZ"
      },
      "outputs": [],
      "source": [
        "# Using the desired format for submission\n",
        "output_data = []\n",
        "for item in preprocessed:\n",
        "  output_data.append({\n",
        "        \"QuestionID\": item['QuestionID'],\n",
        "        \"Question\": item['Question'],\n",
        "        \"RetrievedPassages\": item['RetrievedPassages'],\n",
        "        \"Answer\": \" \".join(item['Obligations']) if item['Obligations'] else \" \".join(item['RetrievedPassages'])\n",
        "    })\n",
        "\n",
        "filepath = 'res'\n",
        "filename = 'NOC_results.json'\n",
        "with open(filepath+'/'+filename, \"w\") as outfile:\n",
        "    json.dump(output_data, outfile, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPX1VRmiI2Hu"
      },
      "source": [
        "# LOC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsS-Evk9IvsV"
      },
      "outputs": [],
      "source": [
        "for question in tqdm(questions):\n",
        "  answers_per_obligation = []\n",
        "  for obligation in question['Obligations']:\n",
        "\n",
        "    answer = ''\n",
        "    coverage_flag = False\n",
        "    tries = 0\n",
        "    while (not coverage_flag) and (tries < 3):\n",
        "      # Generate a partial answer\n",
        "      user_prompt = generate_prompt(input_text=question['Question'], context=obligation, context_type='Obligation')\n",
        "      answer = get_answer(user_prompt=user_prompt, system_prompt=prompts['LOC Prompt'])\n",
        "      # Verify that the sentence produced covers the obligation\n",
        "      coverage_flag = is_covered([obligation], [answer])\n",
        "      tries += 1\n",
        "    answers_per_obligation.append(answer)\n",
        "  question['PartialAnswers'] = answers_per_obligation\n",
        "\n",
        "output_data = []\n",
        "for item in questions:\n",
        "  output_data.append({\n",
        "        \"QuestionID\": item['QuestionID'],\n",
        "        \"Question\": item['Question'],\n",
        "        \"RetrievedPassages\": item['RetrievedPassages'],\n",
        "        \"Answer\": \" \".join(item['PartialAnswers'])\n",
        "    })\n",
        "\n",
        "filepath = \"res\"\n",
        "filename = \"LOC_results\"\n",
        "with open(filepath+'/'+'algorithm2.json', \"w\") as outfile:\n",
        "    json.dump(output_data, outfile, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjtHLukWqB61"
      },
      "outputs": [],
      "source": [
        "# Example of evaluating the NOC algorithm.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  group_methodName = 'NOC' # Replace with your desired method name\n",
        "  input_file = os.path.join(folder_path, \"res/NOC_results.json\") # Replace with your desired system results\n",
        "  main(input_file, group_methodName)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
